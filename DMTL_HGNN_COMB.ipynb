{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PeQTal7NOnm",
        "outputId": "1e319845-fc1d-474f-89ea-1b6c9d24cc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior() \n",
        "from tensorflow.python.framework import function\n",
        "from tensorflow.python.framework import dtypes\n",
        "import numpy.matlib\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xYnpvWyQOHn",
        "outputId": "c66eb096-cb09-4ca7-a42f-0bae713ee67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MTDataset:\n",
        "    def __init__(self, data, label, task_interval, num_class, batch_size):\n",
        "        self.data = data\n",
        "        self.data_dim = data.shape[1]\n",
        "        self.label = np.reshape(label, [1, -1])\n",
        "        self.task_interval = np.reshape(task_interval, [1, -1])\n",
        "        self.num_task = task_interval.size - 1\n",
        "        self.num_class = num_class\n",
        "        self.batch_size = batch_size\n",
        "        self.__build_index__()\n",
        "\n",
        "    def __build_index__(self):\n",
        "        index_list = []\n",
        "        for i in range(self.num_task):\n",
        "            start = self.task_interval[0, i]\n",
        "            end = self.task_interval[0, i + 1]\n",
        "            for j in range(self.num_class):\n",
        "                index_list.append(np.arange(start, end)[np.where(self.label[0, start:end] == j)[0]])\n",
        "        self.index_list = index_list\n",
        "        self.counter = np.zeros([1, self.num_task * self.num_class], dtype=np.int32)\n",
        "\n",
        "    def get_next_batch(self): \n",
        "        sampled_data = np.zeros([self.batch_size * self.num_class * self.num_task, self.data_dim], dtype=np.float32)\n",
        "        sampled_label = np.zeros([self.batch_size * self.num_class * self.num_task, self.num_class], dtype=np.int32)\n",
        "        sampled_task_ind = np.zeros([1, self.batch_size * self.num_class * self.num_task], dtype=np.int32)\n",
        "        sampled_label_ind = np.zeros([1, self.batch_size * self.num_class * self.num_task], dtype=np.int32)\n",
        "        for i in range(self.num_task):\n",
        "            for j in range(self.num_class):\n",
        "                cur_ind = i * self.num_class + j\n",
        "                task_class_index = self.index_list[cur_ind]\n",
        "                sampled_ind = range(cur_ind * self.batch_size, (cur_ind + 1) * self.batch_size)\n",
        "                sampled_task_ind[0, sampled_ind] = i\n",
        "                sampled_label_ind[0, sampled_ind] = j\n",
        "                sampled_label[sampled_ind, j] = 1\n",
        "                if task_class_index.size < self.batch_size:\n",
        "                    sampled_data[sampled_ind, :] = self.data[np.concatenate((task_class_index, task_class_index[\n",
        "                        np.random.randint(0, high=task_class_index.size, size=self.batch_size - task_class_index.size)])), :]\n",
        "                elif self.counter[0, cur_ind] + self.batch_size < task_class_index.size:\n",
        "                    sampled_data[sampled_ind, :] = self.data[task_class_index[self.counter[0, cur_ind]:self.counter[0, cur_ind] + self.batch_size], :]\n",
        "                    self.counter[0, cur_ind] = self.counter[0, cur_ind] + self.batch_size\n",
        "                else:\n",
        "                    sampled_data[sampled_ind, :] = self.data[task_class_index[-self.batch_size:], :]\n",
        "                    self.counter[0, cur_ind] = 0\n",
        "                    np.random.shuffle(self.index_list[cur_ind])\n",
        "        return sampled_data, sampled_label, sampled_task_ind, sampled_label_ind\n",
        "\n",
        "\n",
        "class MTDataset_Split:\n",
        "    def __init__(self, data, label, task_interval, num_class):\n",
        "        self.data = data\n",
        "        print(len(data[-1]),len(data[0]))\n",
        "        print(data.shape)\n",
        "        self.data_dim = data.shape[1]\n",
        "        self.label = np.reshape(label, [1, -1])\n",
        "        self.task_interval = np.reshape(task_interval, [1, -1])\n",
        "        self.num_task = task_interval.size - 1\n",
        "        self.num_class = num_class\n",
        "        self.__build_index__()\n",
        "\n",
        "    def __build_index__(self):\n",
        "        index_list = []\n",
        "        self.num_class_ins = np.zeros([self.num_task, self.num_class])\n",
        "        for i in range(self.num_task):\n",
        "            start = self.task_interval[0, i]\n",
        "            end = self.task_interval[0, i + 1]\n",
        "            for j in range(self.num_class):\n",
        "                index_array = np.where(self.label[0, start:end] == j)[0]\n",
        "                self.num_class_ins[i, j] = index_array.size\n",
        "                index_list.append(np.arange(start, end)[index_array])\n",
        "        self.index_list = index_list\n",
        "\n",
        "    def split(self, train_size):\n",
        "        if train_size < 1:\n",
        "            train_num = np.ceil(self.num_class_ins * train_size).astype(np.int32)\n",
        "        else:\n",
        "            train_num = np.ones([self.num_task, self.num_class], dtype=np.int32) * train_size\n",
        "            train_num = np.maximum(1, np.minimum(train_num, self.num_class_ins - 10))\n",
        "            train_num = train_num.astype(np.int32)\n",
        "        traindata = np.zeros([0, self.data_dim], dtype=np.float32)\n",
        "        testdata = np.zeros([0, self.data_dim], dtype=np.float32)\n",
        "        trainlabel = np.zeros([1, 0], dtype=np.int32)\n",
        "        testlabel = np.zeros([1, 0], dtype=np.int32)\n",
        "        train_task_interval = np.zeros([1, self.num_task + 1], dtype=np.int32)\n",
        "        test_task_interval = np.zeros([1, self.num_task + 1], dtype=np.int32)\n",
        "        for i in range(self.num_task):\n",
        "            for j in range(self.num_class):\n",
        "                cur_ind = i * self.num_class + j\n",
        "                task_class_index = self.index_list[cur_ind]\n",
        "                np.random.shuffle(task_class_index)\n",
        "                train_index = task_class_index[0:train_num[i, j]]\n",
        "                test_index = task_class_index[train_num[i, j]:]\n",
        "                traindata = np.concatenate((traindata, self.data[train_index, :]), axis=0)\n",
        "                trainlabel = np.concatenate((trainlabel, np.ones([1, train_index.size], dtype=np.int32) * j), axis=1)\n",
        "                testdata = np.concatenate((testdata, self.data[test_index, :]), axis=0)\n",
        "                testlabel = np.concatenate((testlabel, np.ones([1, test_index.size], dtype=np.int32) * j), axis=1)\n",
        "            train_task_interval[0, i + 1] = trainlabel.size\n",
        "            test_task_interval[0, i + 1] = testlabel.size\n",
        "        return traindata, trainlabel, train_task_interval, testdata, testlabel, test_task_interval\n",
        "\n",
        "\n",
        "def read_data_from_file(filename):\n",
        "    file = open(filename, 'r')\n",
        "    contents = file.readlines()\n",
        "    file.close()\n",
        "    num_task = int(contents[0])\n",
        "    num_class = int(contents[1])\n",
        "    temp_ind = re.split(',', contents[2])\n",
        "    temp_ind = [int(elem) for elem in temp_ind]\n",
        "    task_interval = np.reshape(np.array(temp_ind), [1, -1])\n",
        "    temp_data = []\n",
        "    for pos in range(3, len(contents) - 1):\n",
        "        temp_sub_data = re.split(',', contents[pos])\n",
        "        temp_sub_data = [float(elem) for elem in temp_sub_data]\n",
        "        temp_data.append(temp_sub_data)\n",
        "    data = np.array(temp_data)\n",
        "    temp_label = re.split(',', contents[-1])\n",
        "    temp_label = [int(elem) for elem in temp_label]\n",
        "    label = np.reshape(np.array(temp_label), [1, -1])\n",
        "    return data, label, task_interval, num_task, num_class\n",
        "\n",
        "\n",
        "def compute_train_loss(i, feature_representation, hidden_output_weight, inputs_data_label, inputs_task_ind, inputs_num_ins_per_task, train_loss):\n",
        "    train_loss += tf.div(tf.losses.softmax_cross_entropy(tf.expand_dims(inputs_data_label[i, :], 0),\n",
        "                                                         tf.matmul(tf.expand_dims(feature_representation[inputs_task_ind[0, i]][i % (batch_size * inputs_data_label.shape[-1])][:], 0),\n",
        "                                                                   hidden_output_weight[inputs_task_ind[0, i], :, :])),\n",
        "                         tf.cast(inputs_num_ins_per_task[0, inputs_task_ind[0, i]], dtype=tf.float32))\n",
        "    return i + 1, feature_representation, hidden_output_weight, inputs_data_label, inputs_task_ind, inputs_num_ins_per_task, train_loss\n",
        "\n",
        "\n",
        "def gradient_clipping_tf_false_consequence(optimizer, obj, gradient_clipping_threshold):\n",
        "    gradients, variables = zip(*optimizer.compute_gradients(obj))\n",
        "    gradients = [None if gradient is None else tf.clip_by_value(gradient, gradient_clipping_threshold,\n",
        "                                                                tf.negative(gradient_clipping_threshold)) for gradient in gradients]\n",
        "    train_step = optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return train_step\n",
        "\n",
        "\n",
        "def gradient_clipping_tf(optimizer, obj, option, gradient_clipping_threshold):\n",
        "    train_step = tf.cond(tf.equal(option, 0), lambda: optimizer.minimize(obj),\n",
        "                         lambda: gradient_clipping_tf_false_consequence(optimizer, obj, gradient_clipping_threshold))\n",
        "    train_step = tf.group(train_step)\n",
        "    return train_step\n",
        "\n",
        "\n",
        "def generate_label_task_ind(label, task_interval, num_class):\n",
        "    num_task = task_interval.size - 1\n",
        "    num_ins = label.size\n",
        "    label_matrix = np.zeros((num_ins, num_class), dtype=np.int32)\n",
        "    label_matrix[range(num_ins), label] = 1\n",
        "    task_ind = np.zeros((1, num_ins), dtype=np.int32)\n",
        "    for i in range(num_task):\n",
        "        task_ind[0, task_interval[0, i]: task_interval[0, i + 1]] = i\n",
        "    return label_matrix, task_ind\n",
        "\n",
        "\n",
        "def compute_errors(hidden_rep, hidden_output_weight, task_ind, label, num_task):\n",
        "    num_total_ins = hidden_rep.shape[0]\n",
        "    num_ins = np.zeros([1, num_task])\n",
        "    errors = np.zeros([1, num_task + 1])\n",
        "    for i in range(num_total_ins):\n",
        "        probit = np.matmul(hidden_rep[i, :], hidden_output_weight[task_ind[0, i], :, :])\n",
        "        num_ins[0, task_ind[0, i]] += 1\n",
        "        if np.argmax(probit) != label[0, i]:\n",
        "            errors[0, task_ind[0, i]] += 1\n",
        "    for i in range(num_task):\n",
        "        errors[0, i] = errors[0, i] / num_ins[0, i]\n",
        "    errors[0, num_task] = np.mean(errors[0, 0: num_task])\n",
        "    return errors\n",
        "\n",
        "\n",
        "def change_datastruct(hidden_features, num_task):\n",
        "    return tf.reshape(hidden_features, [num_task, -1, hidden_features.shape[-1]])\n",
        "\n",
        "\n",
        "def compute_pairwise_dist_tf(data):\n",
        "    sq_data_norm = tf.reduce_sum(tf.square(data), axis=1)\n",
        "    sq_data_norm = tf.reshape(sq_data_norm, [-1, 1])\n",
        "    dist_matrix = sq_data_norm - 2 * tf.matmul(data, data, transpose_b=True) + tf.matrix_transpose(sq_data_norm)\n",
        "    return dist_matrix\n",
        "\n",
        "\n",
        "def compute_pairwise_dist_np(data):\n",
        "    sq_data_norm = np.sum(data ** 2, axis=1)\n",
        "    sq_data_norm = np.reshape(sq_data_norm, [-1, 1])\n",
        "    dist_matrix = sq_data_norm - 2 * np.dot(data, data.transpose()) + sq_data_norm.transpose()\n",
        "    return dist_matrix\n",
        "\n",
        "\n",
        "def compute_adjacency_matrix(hidden_features, inputs_data_label, num_task):\n",
        "    new_hidden_features = change_datastruct(hidden_features, num_task)\n",
        "    new_inputs_data_label = change_datastruct(inputs_data_label, num_task)\n",
        "    adjacency_matrixs = []\n",
        "    for i in range(num_task):\n",
        "        dist_matrix = -compute_pairwise_dist_tf(new_hidden_features[i])\n",
        "        sign_matrix = 2 * tf.matmul(new_inputs_data_label[i], tf.matrix_transpose(new_inputs_data_label[i])) - 1\n",
        "        adjacency_matrix = tf.exp(dist_matrix) * sign_matrix\n",
        "        adjacency_matrixs.append(adjacency_matrix)\n",
        "    adjacency_matrixs = tf.stack(adjacency_matrixs)\n",
        "    return adjacency_matrixs\n",
        "\n",
        "\n",
        "def activate_function(temp, activate_op):\n",
        "    if activate_op == 1:\n",
        "        return tf.tanh(temp)\n",
        "    elif activate_op == 2:\n",
        "        return tf.nn.relu(temp)\n",
        "    elif activate_op == 3:\n",
        "        return tf.nn.elu(temp)\n",
        "    else:\n",
        "        return\n",
        "\n",
        "\n",
        "def get_normed_distance_tf(data):\n",
        "    norminator = tf.matmul(data, tf.transpose(data))\n",
        "    square = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(data), 1)), [norminator.shape[0], 1])\n",
        "    denorminator = tf.matmul(square, tf.transpose(square))\n",
        "    return norminator/denorminator\n",
        "\n",
        "\n",
        "def get_normed_distance_np(data):\n",
        "    norminator = np.matmul(data, np.transpose(data))\n",
        "    square = np.reshape(np.sqrt(np.sum(np.square(data), 1)), [norminator.shape[0], 1])\n",
        "    denorminator = np.matmul(square, np.transpose(square))\n",
        "    return norminator/denorminator\n",
        "\n",
        "\n",
        "def GAT(attention_weight, embedding_vectors):\n",
        "    transformaed_embedding_vectors = tf.matmul(embedding_vectors, attention_weight)\n",
        "    attention_values = tf.nn.softmax(get_normed_distance_tf(transformaed_embedding_vectors))\n",
        "    return attention_values\n",
        "\n",
        "\n",
        "def get_feature_representation(inputs, input_hidden_weights, hidden_features, adjacency_matrix, num_task, num_class, activate_op,\n",
        "                            first_task_att_w, first_class_att_w, task_attention_weight, class_attention_weight, inputs_data_label):\n",
        "    hidden_representation = activate_function(\n",
        "        tf.add(change_datastruct(tf.matmul(inputs, input_hidden_weights), num_task),\n",
        "               tf.matmul(adjacency_matrix, change_datastruct(hidden_features, num_task))), activate_op)\n",
        "\n",
        "    new_inputs_data_label = change_datastruct(inputs_data_label, num_task)\n",
        "    new_adjacency_matrix = []\n",
        "    for i in range(num_task):\n",
        "        dist_matrix = -compute_pairwise_dist_tf(hidden_representation[i])\n",
        "        sign_matrix = 2 * tf.matmul(new_inputs_data_label[i], tf.matrix_transpose(new_inputs_data_label[i])) - 1\n",
        "        adjacency_matrix = tf.exp(dist_matrix) * sign_matrix\n",
        "        new_adjacency_matrix.append(adjacency_matrix)\n",
        "    new_adjacency_matrix = tf.stack(new_adjacency_matrix)\n",
        "    new_hidden_representation = activate_function(\n",
        "        tf.add(change_datastruct(tf.matmul(inputs, input_hidden_weights), num_task),\n",
        "               tf.matmul(new_adjacency_matrix, hidden_representation)), activate_op)\n",
        "\n",
        "    task_embedding_vectors = tf.reduce_max(new_hidden_representation, 1)\n",
        "    task_attention_values = GAT(first_task_att_w, task_embedding_vectors)\n",
        "    new_task_embedding_vectors = tf.tanh(tf.matmul(task_attention_values, tf.matmul(task_embedding_vectors, first_task_att_w)))\n",
        "    task_attention_values = GAT(task_attention_weight, new_task_embedding_vectors)\n",
        "    new_task_embedding_vectors = tf.tanh(tf.matmul(task_attention_values, tf.matmul(new_task_embedding_vectors, task_attention_weight)))\n",
        "\n",
        "    class_embedding_vectors = []\n",
        "    for i in range(num_task):\n",
        "        class_hidden_rep = tf.reshape(new_hidden_representation[i], [num_class, -1, new_hidden_representation.shape[-1]])\n",
        "        for j in range(num_class):\n",
        "            class_embedding_vectors.append(tf.reduce_max(class_hidden_rep[j], 0))\n",
        "    class_embedding_vectors = tf.stack(class_embedding_vectors)\n",
        "    class_attention_values = GAT(first_class_att_w, class_embedding_vectors)\n",
        "    new_class_embedding_vectors = tf.tanh(tf.matmul(class_attention_values, tf.matmul(class_embedding_vectors, first_class_att_w)))\n",
        "    class_attention_values = GAT(class_attention_weight, new_class_embedding_vectors)\n",
        "    new_class_embedding_vectors = tf.tanh(tf.matmul(class_attention_values, tf.matmul(new_class_embedding_vectors, class_attention_weight)))\n",
        "\n",
        "    feature_representations = []\n",
        "    for i in range(num_task):\n",
        "        feature_representation = []\n",
        "        feature_representation_1 = tf.concat([\n",
        "            hidden_features[i * batch_size * num_class: (i + 1) * batch_size * num_class],\n",
        "            tf.stack([new_task_embedding_vectors[i] for _ in range(num_class * batch_size)])], 1)\n",
        "        for j in range(num_class):\n",
        "            feature_representation_2 = tf.concat([\n",
        "                feature_representation_1[j * batch_size: (j + 1) * batch_size],\n",
        "                tf.stack([new_class_embedding_vectors[i * num_task + j] for _ in range(batch_size)])], 1)\n",
        "            feature_representation.append(feature_representation_2)\n",
        "        feature_representations.append(feature_representation)\n",
        "    feature_representations = tf.stack(feature_representations)\n",
        "    return tf.reshape(feature_representations, [num_task, -1, hidden_features.shape[-1] + F_pie_t + F_pie_c])\n",
        "\n",
        "\n",
        "def np_softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    exp_x = np.exp(x)\n",
        "    softmax_x = exp_x / np.sum(exp_x)\n",
        "    return softmax_x\n",
        "\n",
        "\n",
        "def get_embedding_vec(traindata, input_hidden_weights, first_task_att_w, first_class_att_w, task_attention_weight,\n",
        "                      class_attention_weight, train_hidden_features, train_label_matrix, train_task_ind, train_num_ins_per_task, num_task,  num_class):\n",
        "    inputs = [[] for _ in range(num_task)]\n",
        "    features = [[] for _ in range(num_task)]\n",
        "    labels = [[] for _ in range(num_task)]\n",
        "    for i in range(traindata.shape[0]):\n",
        "        inputs[train_task_ind[0, i]].append(traindata[i])\n",
        "        features[train_task_ind[0, i]].append(train_hidden_features[i])\n",
        "        labels[train_task_ind[0, i]].append(train_label_matrix[i])\n",
        "    task_embedding_vectors = []\n",
        "    class_embedding_vectors = []\n",
        "    for i in range(num_task):\n",
        "        dist_matrix = -compute_pairwise_dist_np(np.stack(features[i]))\n",
        "        sign_matrix = 2 * np.matmul(np.stack(labels[i]), np.transpose(np.stack(labels[i]))) - 1\n",
        "        adjacency_matrix = np.exp(dist_matrix) * sign_matrix\n",
        "        new_features = np.tanh(np.add(np.matmul(np.stack(inputs[i]), input_hidden_weights),\n",
        "                                      np.matmul(adjacency_matrix, np.stack(features[i]))))\n",
        "        new_dist_matrix = -compute_pairwise_dist_np(new_features)\n",
        "        new_adjacency_matrix = np.exp(new_dist_matrix) * sign_matrix\n",
        "        new_features = np.tanh(np.add(np.matmul(np.stack(inputs[i]), input_hidden_weights),\n",
        "                                      np.matmul(new_adjacency_matrix, new_features)))\n",
        "\n",
        "        task_embedding_vector = np.max(new_features, 0)\n",
        "        task_embedding_vectors.append(task_embedding_vector)\n",
        "        inputs_class = [[] for _ in range(num_class)]\n",
        "        features_class = [[] for _ in range(num_class)]\n",
        "        labels_class = [[] for _ in range(num_class)]\n",
        "        for j in range(len(inputs[i])):\n",
        "            inputs_class[np.int(np.argmax(labels[i][j]))].append(inputs[i][j])\n",
        "            features_class[np.int(np.argmax(labels[i][j]))].append(features[i][j])\n",
        "            labels_class[np.int(np.argmax(labels[i][j]))].append(labels[i][j])\n",
        "        for j in range(num_class):\n",
        "            dist_matrix = -compute_pairwise_dist_np(np.stack(features_class[j]))\n",
        "            sign_matrix = 2 * np.matmul(np.stack(labels_class[j]), np.transpose(np.stack(labels_class[j]))) - 1\n",
        "            adjacency_matrix = np.exp(dist_matrix) * sign_matrix\n",
        "            new_features = np.tanh(np.add(np.matmul(np.stack(inputs_class[j]), input_hidden_weights),\n",
        "                                          np.matmul(adjacency_matrix, np.stack(features_class[j]))))\n",
        "            class_embedding_vector = np.max(new_features, 0)\n",
        "            class_embedding_vectors.append(class_embedding_vector)\n",
        "    task_attention_values = np_softmax(get_normed_distance_np(np.stack(task_embedding_vectors)))\n",
        "    new_task_embedding_vectors = np.tanh(np.matmul(task_attention_values, np.matmul(task_embedding_vectors, first_task_att_w)))\n",
        "    task_attention_values = np_softmax(get_normed_distance_np(np.stack(new_task_embedding_vectors)))\n",
        "    new_task_embedding_vectors = np.tanh(np.matmul(task_attention_values, np.matmul(new_task_embedding_vectors, task_attention_weight)))\n",
        "\n",
        "    class_attention_values = np_softmax(get_normed_distance_np(np.stack(class_embedding_vectors)))\n",
        "    new_class_embedding_vectors = np.tanh(np.matmul(class_attention_values, np.matmul(class_embedding_vectors, first_class_att_w)))\n",
        "    class_attention_values = np_softmax(get_normed_distance_np(np.stack(new_class_embedding_vectors)))\n",
        "    new_class_embedding_vectors = np.tanh(np.matmul(class_attention_values, np.matmul(new_class_embedding_vectors, class_attention_weight)))\n",
        "\n",
        "    return new_task_embedding_vectors, new_class_embedding_vectors\n",
        "\n",
        "\n",
        "def get_new_hidden_features(test_hidden_rep, task_embedding_vectors, class_embedding_vectors, hidden_output_weight, test_task_ind, num_task, num_class):\n",
        "    temp_test_hidden_rep = []\n",
        "    for i in range(test_hidden_rep.shape[0]):\n",
        "        temp = np.concatenate([test_hidden_rep[i], task_embedding_vectors[test_task_ind[0, i]]], 0)\n",
        "        temp_test_hidden_rep.append(temp)\n",
        "    temp_test_hidden_rep = np.stack(temp_test_hidden_rep)\n",
        "    test_hidden_rep = []\n",
        "    for i in range(len(temp_test_hidden_rep)):\n",
        "        task_id = test_task_ind[0, i]\n",
        "        probits_softmax = []\n",
        "        for j in range(num_class):\n",
        "            temp = np.concatenate([temp_test_hidden_rep[i], class_embedding_vectors[task_id * num_task + j]], 0)\n",
        "            probit_softmax = np_softmax(np.matmul(temp, hidden_output_weight[task_id]))\n",
        "            probits_softmax.append(probit_softmax)\n",
        "        probits_softmax = np.stack(probits_softmax)\n",
        "        diagonal = []\n",
        "        for j in range(num_class):\n",
        "            diagonal.append(probits_softmax[j][j])\n",
        "        class_id = np.argmax(diagonal)\n",
        "        test_hidden_rep.append(np.concatenate([temp_test_hidden_rep[i], class_embedding_vectors[task_id * num_class + class_id]], 0))\n",
        "    test_hidden_rep = np.stack(test_hidden_rep)\n",
        "    return test_hidden_rep\n",
        "\n",
        "\n",
        "def DMTL_HGNN(traindata, trainlabel, train_task_interval, dim, num_class, num_task, hidden_dim, batch_size, reg_para,\n",
        "         max_epoch, testdata, testlabel, test_task_interval, activate_op):\n",
        "    print('DMTL_HGNN is running...')\n",
        "    inputs = tf.placeholder(tf.float32, shape=[None, dim])\n",
        "    inputs_data_label = tf.placeholder(tf.float32, shape=[None, num_class])\n",
        "    inputs_task_ind = tf.placeholder(tf.int32, shape=[1, None])\n",
        "    inputs_num_ins_per_task = tf.placeholder(tf.int32, shape=[1, None])\n",
        "    input_hidden_weights = tf.Variable(tf.truncated_normal([dim, hidden_dim], dtype=tf.float32, stddev=1e-1))\n",
        "    hidden_features = activate_function(tf.matmul(inputs, input_hidden_weights), activate_op)\n",
        "    adjacency_matrix = compute_adjacency_matrix(hidden_features, inputs_data_label, num_task)\n",
        "\n",
        "    first_task_att_w = tf.Variable(tf.truncated_normal(\n",
        "        [hidden_dim, GAT_hidden_dim], dtype=tf.float32, stddev=1e-1))\n",
        "    first_class_att_w = tf.Variable(tf.truncated_normal(\n",
        "        [hidden_dim, GAT_hidden_dim], dtype=tf.float32, stddev=1e-1))\n",
        "    task_attention_weight = tf.Variable(tf.truncated_normal(\n",
        "        [GAT_hidden_dim, F_pie_t], dtype=tf.float32, stddev=1e-1))\n",
        "    class_attention_weight = tf.Variable(tf.truncated_normal(\n",
        "        [GAT_hidden_dim, F_pie_c], dtype=tf.float32, stddev=1e-1))\n",
        "\n",
        "    feature_representation = get_feature_representation(inputs, input_hidden_weights, hidden_features, adjacency_matrix,\n",
        "                                               num_task, num_class, activate_op, first_task_att_w, first_class_att_w, task_attention_weight, class_attention_weight, inputs_data_label)\n",
        "\n",
        "    hidden_output_weight = tf.Variable(tf.truncated_normal(\n",
        "        [num_task, hidden_dim + F_pie_t + F_pie_c, num_class], dtype=tf.float32, stddev=1e-1))\n",
        "\n",
        "    train_loss = tf.Variable(0.0, dtype=tf.float32)\n",
        "    _, _, _, _, _, _, train_loss = tf.while_loop(\n",
        "        cond=lambda i, j1, j2, j3, j4, j5, j6: tf.less(i, tf.shape(inputs_task_ind)[1]), body=compute_train_loss,\n",
        "        loop_vars=(tf.constant(0, dtype=tf.int32), feature_representation, hidden_output_weight,\n",
        "                   inputs_data_label, inputs_task_ind, inputs_num_ins_per_task, train_loss))\n",
        "\n",
        "    obj = train_loss + reg_para * (tf.square(tf.norm(input_hidden_weights))+tf.square(tf.norm(hidden_output_weight)))\n",
        "\n",
        "    learning_rate = tf.placeholder(tf.float32)\n",
        "    gradient_clipping_threshold = tf.placeholder(tf.float32)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    gradient_clipping_option = tf.placeholder(tf.int32)\n",
        "    train_step = gradient_clipping_tf(optimizer, obj, gradient_clipping_option, gradient_clipping_threshold)\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    with tf.Session() as sess:\n",
        "        max_iter_epoch = numpy.ceil(traindata.shape[0] / (batch_size * num_task * num_class)).astype(\n",
        "            np.int32)\n",
        "        Iterator = MTDataset(traindata, trainlabel, train_task_interval, num_class, batch_size)\n",
        "        sess.run(init_op)\n",
        "\n",
        "        train_label_matrix, train_task_ind = generate_label_task_ind(trainlabel, train_task_interval, num_class)\n",
        "        for iter in range(max_iter_epoch * max_epoch):\n",
        "            sampled_data, sampled_label, sampled_task_ind, _ = Iterator.get_next_batch()\n",
        "            num_iter = iter // max_iter_epoch\n",
        "            train_step.run(feed_dict={d1: d2 for d1, d2 in\n",
        "                                      zip([learning_rate, gradient_clipping_option, gradient_clipping_threshold, inputs,\n",
        "                                           inputs_data_label, inputs_task_ind, inputs_num_ins_per_task],\n",
        "                                          [0.02 / (1 + num_iter), 0, -5., sampled_data, sampled_label, sampled_task_ind,\n",
        "                                           np.ones([1, num_task]) * (batch_size * num_class)])})\n",
        "            if iter % max_iter_epoch == 0 and num_iter % 5 == 0:\n",
        "                train_hidden_features = hidden_features.eval(feed_dict={inputs: traindata, inputs_task_ind: train_task_ind})\n",
        "                task_embedding_vectors, class_embedding_vectors = get_embedding_vec(traindata, input_hidden_weights.eval(), first_task_att_w.eval(), first_class_att_w.eval(), task_attention_weight.eval(), class_attention_weight.eval(),\n",
        "                                    train_hidden_features, train_label_matrix, train_task_ind, np.reshape(\n",
        "                                   train_task_interval[0, 1:] - train_task_interval[0, 0:num_task], [1, -1]), num_task, num_class)\n",
        "                _, test_task_ind = generate_label_task_ind(testlabel, test_task_interval, num_class)\n",
        "                test_hidden_rep = hidden_features.eval(feed_dict={inputs: testdata, inputs_task_ind: test_task_ind})\n",
        "                new_test_hidden_rep = get_new_hidden_features(test_hidden_rep, task_embedding_vectors, class_embedding_vectors, hidden_output_weight.eval(), test_task_ind, num_task, num_class)\n",
        "                test_errors = compute_errors(new_test_hidden_rep, hidden_output_weight.eval(), test_task_ind, testlabel,\n",
        "                                                 num_task)\n",
        "                print('epoch = %g, test_errors = %s' % (num_iter, test_errors))\n",
        "    return test_errors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cpqlnrnBY0Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_process(filename, train_size, hidden_dim, batch_size, reg_para, max_epoch, use_gpu, gpu_id='0', activate_op=1):\n",
        "    if use_gpu == 1:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\n",
        "    else:\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "    data, label, task_interval, num_task, num_class = read_data_from_file(filename)\n",
        "    for i in range(len(data)):\n",
        "        if len(data[i])!=27000:\n",
        "            print(i,len(data[i]))\n",
        "    data_split = MTDataset_Split(data, label, task_interval, num_class)\n",
        "    dim = data.shape[1]\n",
        "    traindata, trainlabel, train_task_interval, testdata, testlabel, test_task_interval = data_split.split(train_size)\n",
        "    error = DMTL_HGNN(traindata, trainlabel, train_task_interval, dim, num_class, num_task, hidden_dim, batch_size, reg_para, max_epoch, testdata, testlabel, test_task_interval, activate_op)\n",
        "    return error\n",
        "\n",
        "\n",
        "datafile = '/content/drive/MyDrive/maskeddata/img-text-mt.txt'\n",
        "max_epoch = 50\n",
        "use_gpu = 1\n",
        "gpu_id = '2'\n",
        "hidden_dim = 600\n",
        "batch_size = 32\n",
        "reg_para = 0.2\n",
        "train_size = 0.7\n",
        "activate_op = 1\n",
        "GAT_hidden_dim = 16\n",
        "F_pie_t = 8\n",
        "F_pie_c = 8\n",
        "\n",
        "mean_errors = main_process(datafile, train_size, hidden_dim, batch_size, reg_para, max_epoch, use_gpu, gpu_id,\n",
        "                           activate_op)\n",
        "\n",
        "print('final test_errors = ', mean_errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7-rEH4vY5mn",
        "outputId": "ae9b0c7d-671f-444d-e834-283b7c97fca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27000 27000\n",
            "(1577, 27000)\n",
            "DMTL_HGNN is running...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-0a09b5b47ed1>:319: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  inputs_class[np.int(np.argmax(labels[i][j]))].append(inputs[i][j])\n",
            "<ipython-input-4-0a09b5b47ed1>:320: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  features_class[np.int(np.argmax(labels[i][j]))].append(features[i][j])\n",
            "<ipython-input-4-0a09b5b47ed1>:321: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels_class[np.int(np.argmax(labels[i][j]))].append(labels[i][j])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "epoch = 5, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "epoch = 10, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "epoch = 15, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "epoch = 20, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "epoch = 25, test_errors = [[0.43814433 0.42929293 0.43371863]]\n",
            "final test_errors =  [[0.43814433 0.42929293 0.43371863]]\n"
          ]
        }
      ]
    }
  ]
}